# Лабораторная работа №2
  
## Дано  

1. Несколько научных статей и текст о жизни первого космонавта Юрия Гагарина на русском языке. Все они находятся в папке [assets](./assets).
2. Список стоп слов (предлоги, союзы, местоимения и другие неполнозначные слова русского языка). [Ссылка](./assets/stop_words.txt). 
    * Данный список взят из библиотеки для работы с естественным языком [NLTK](https://www.nltk.org/index.html).

Необходимо выделить ключевые фразы из нескольких текстов: биографической статьи и научных статей.

Код, считывающий все перечисленные выше материалы, уже написан для вас в `start.py`.

Вы найдете эти ресурсы в следующих переменных:
* `corpus` - словарь, ключами в котором являются названия текстов, а значениями сами тексты. Ключи:
  * `gagarin` - текст о жизни первого космонавта [Юрия Гагарина](https://ru.wikipedia.org/wiki/Гагарин,_Юрий_Алексеевич)
  * `albatross` - текст об [альбатросах](https://nplus1.ru/blog/2022/06/14/eye-of-the-albatross)
  * `genome_engineering` - текст об [изменении генома человека](https://nplus1.ru/blog/2022/05/05/manniskan-i-provroret)
  * `pain_detection` - текст о [распознавании уровня боли](https://nplus1.ru/news/2022/09/05/mouse-pain-face)
* `stop_words` - список стоп слов

**Важно:** В рамках данной лабораторной работы **нельзя использовать сторонние модули и модуль collections.**

**Важно:** Вы *можете* использовать функции из предыдущих лабораторных работ.

Следующее правило применяется ко всем функциям, которые необходимо реализовать в лабораторной работе:

Если на вход в функцию подаются некорректные аргументы, возвращается `None`.
  * Некорректным аргумент считается в следующих случаях:
    * его тип отличается от ожидаемого типа
    * у него нет содержимого

## Терминология

В данной работе вы встретитесь с терминами *ключевые слова*, *ключевые фразы* и некоторыми другими, которые будут объяснены в соответствующих секциях.

* Ключевое слово - слово в тексте, способное в совокупности с другими ключевыми словами дать высокоуровневое описание содержания текстового документа, выявить его тематику. [Источник](https://ru.wikipedia.org/wiki/Ключевое_слово). 
* Ключевая фраза - несколько слов в связке, которые дают высокоуровневое описание содержания текстового документа.
    * Ключевая фраза может быть длиной в несколько слов или в одно. То есть ключевое слово это частный случай ключевой фразы.
    * Ключевая фраза **не обязана** состоять только из ключевых слов - это можно будет увидеть в примерах в следующих секциях.

Пример для научной статьи о геноме:

* Ключевая фраза: _редактирование генома_
* Ключевые слова: _учёные_, _вирусы_, _геном_

## Подход к выделению ключевых фраз

У большинства задач есть несколько решений. Это применимо и к выделению ключевых слов. 

В [лабораторной работе №1](../lab_1_keywords_tfidf/README.md) вы познакомились с выделением ключевых слов с помощью метрики TF-IDF и $\chi^2$. 
Этот метод выделял отдельные ключевые слова из документа с помощью сравнивания частотностей слов в документе и корпусе.

У этого метода можно выделить следующие недостатки:
* Требуются посчитанные значения IDF, причём посчитанные на достаточно большом корпусе с соответствующей тематикой;
    * Для каждого языка эти значения должны быть посчитаны заново.
* Выделяются _отдельные_ ключевые слова, когда как словосочетания могут дать больше информации о документе.

Существует алгоритм, позволяющий выделить ключевые слова **и** ключевые фразы из документа, требуя только сам документ и 
список стоп слов для языка.

Данный алгоритм называется [**_RAKE_**](https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents) (Rapid Automatic Keyword Extraction). Кратко принцип его работы можно описать так:

1. Текст документа разбивается на отдельные фразы по разделителям
    * Разделителями являются знаки препинания (`.,:;()` и т.д.)
2. Фразы токенизируются (разделяются на отдельные слова)
3. Из каждой фразы выделяются возможные ключевые фразы
    * Каждая фраза разбивается по позициям стоп слов
4. Для каждого слова считаются метрики значимости
    * Word Frequency, Word Degree и Word Score
5. Для каждой возможной ключевой фразы считается метрика значимости
    * Как сумма метрик значимости для каждого слова в возможной ключевой фразе
6. Возможные ключевые фразы ранжируются по метрике значимости
7. Выделяется топ N возможных ключевых фраз - они и будут являться ключевыми фразами для документа

В данной лабораторной работе вы реализуете данный алгоритм, позволяющий выделять _ключевые фразы_. 
Каждый из представленных выше шагов более подробно описан в соответствующей секции.

**Важно**: вы могли заметить, что в названии алгоритма используется слово _keyword_, которое переводится как _ключевое слово_. 
Однако, в самом алгоритме выделяются не только отдельно стоящие _ключевые слова_, но и целые _ключевые фразы_. 

Чтобы избежать путаницы, здесь и далее мы будем говорить о выделении _ключевых фраз_, которые могут состоять как из одного слова, 
так и из нескольких.

## Что надо сделать  

### Шаг 0. Подготовка (проделать вместе с преподавателем на практике).  
  
1. Изменить файлы `main.py` и `start.py`  
2. Закоммитить изменения и создать новый pull request
  
**Важно:** Код, выполняющий все действия от предобработки текста до выделения ключевых слов, должен быть написан в `start.py`. 
Для этого реализуйте функции в модуле `main.py` и импортируйте их в `start.py`.
  
```py  
if __name__ == '__main__':  
 # your code goes here
```

### Шаг 1. Выделить из текста фразы

Функция принимает на вход текст в виде строки. 

Возвращаемым значением функции должен быть список фраз. 

Фразы выделяются с помощью разбиения текста на отдельные части по [знакам препинания](https://ru.wikipedia.org/wiki/Знаки_препинания), находящимся **вне** слов.

**Пример**:
* Текст: `Во времена Советского Союза исследование космоса было одной из важнейших задач. И именно из СССР была отправлена ракета, совершившая первый полет в космос. Произошло это 12 апреля 1961 года.`
* Список фраз: `['Во времена Советского Союза исследование космоса было одной из важнейших задач', 'И именно из СССР была отправлена ракета', 'совершившая первый полет в космос', 'Произошло это 12 апреля 1961 года']`

Интерфейс:   
```py
def extract_phrases(text: str) -> Optional[list[str]]: 
    pass
```

### Шаг 2. Выделить возможные ключевые фразы из фраз. Выполнение Шагов 1-2 соответствует 4 баллам

Функция принимает на вход список фраз и список стоп слов.  
  
Возвращаемым значением функции должен быть список из возможных ключевых фраз.
* Они называются *возможными*, так как мы ещё не знаем, действительно ли они являются ключевыми для данного текста.

Возможные ключевые фразы из фразы выделяются с помощью приведения фразы к нижнему регистру и разбивки на отдельные части по позициям стоп слов.

**Пример для одной фразы**:
* Фраза: `'Во времена Советского Союза исследование космоса было одной из важнейших задач'`
* Список стоп слов: `['во', 'из', 'было', 'и', 'к', ...]`
* Возможные ключевые фразы: `[('времена', 'советского', 'союза', 'исследование', 'космоса'), ('одной',), ('важнейших', 'задач')]`

Интерфейс:   
```py
def extract_candidate_keyword_phrases(phrases: list[str],
                                      stop_words: list[str]) -> Optional[KeyPhrases]:
    pass
```
 
**Важно:** список возможных ключевых фраз не должен содержать "пустых" фраз - минимальная длина возможной ключевой фразы равна 1 слову.

**Задача:** выделите возможные ключевые фразы из каждого текста корпуса, оцените, насколько правильно произошло выделение. 

**Важно:** все эти шаги нужно выполнять в файле [`start.py`](./start.py).

### Шаг 3. Рассчитать встречаемость каждого знаменательного слова  

Функция принимает на вход список возможных ключевых фраз.  
  
Возвращаемым значением функции должен быть частотный словарь. 

Формат словаря: `{токен: количество вхождений в возможные ключевые слова}`. 

**Пример**:
* Возможные ключевые фразы: `[('времена', 'советского', 'союза', 'исследование', 'космоса'), ('одной',), ('важнейших', 'задач')]`
* Частотный словарь: `{'времена': 1, 'советского': 1, 'союза': 1, 'исследование': 1, 'космоса': 1, 'одной': 1, 'важнейших': 1, 'задач': 1}`

Так как возможные ключевые фразы не содержат стоп слов, то мы можем считать, что в них находятся только знаменательные слова.

Интерфейс:
```py
def calculate_frequencies_for_content_words(candidate_keyword_phrases: KeyPhrases) -> Optional[dict[str, int]]:
    pass
```    

### Шаг 4. Рассчитать Word Degree для каждого слова

Word Degree - статистическая мера, используемая для оценки важности слова в контексте документа. 
Она рассчитывается как сумма количества раз, которое это слово встречается в возможных ключевых фразах во всём документе.

Чем чаще слово встречается в длинных возможных ключевых фразах, тем более оно значимо.

Чтобы понять, как часто слова встречаются рядом друг с другом - то есть в одних и тех же возможных ключевых фразах - 
нужно построить матрицу совместной встречаемости.

**Пример**:
* Возможные ключевые фразы: `[('времена', 'советского', 'союза', 'исследование', 'космоса'), ('одной',), ('важнейших', 'задач'), ('времена', 'союза', 'прошли')]`
* Частотный словарь: `{'времена': 2, 'советского': 1, 'союза': 2, 'исследование': 1, 'космоса': 1, 'одной': 1, 'важнейших': 1, 'задач': 1, 'прошли': 1}`
* Матрица совместной встречаемости для указанных выше данных будет выглядеть следующим образом:
  * Размер: 9x9, где 9 - количество значимых слов;

|              | времена | советского | союза | исследование | космоса | одной | важнейших | задач | прошли |
|--------------|---------|------------|-------|--------------|---------|-------|-----------|-------|--------|
| времена      | 2       | 1          | 2     | 1            | 1       |       |           |       | 1      |
| советского   | 1       | 1          | 1     | 1            | 1       |       |           |       |        |
| союза        | 2       | 1          | 2     | 1            | 1       |       |           |       | 1      |
| исследование | 1       | 1          | 1     | 1            | 1       |       |           |       |        |
| космоса      | 1       | 1          | 1     | 1            | 1       |       |           |       |        |
| одной        |         |            |       |              |         | 1     |           |       |        |
| важнейших    |         |            |       |              |         |       | 1         | 1     |        |
| задач        |         |            |       |              |         |       | 1         |       |        |
| прошли       | 1       |            |       |              |         |       |           |       | 1      |


Объяснение некоторых значений таблицы:
* Слова `времена` и `союза` встречаются в возможных ключевых фразах по два раза, соответственно, в их ячейках значение 2.
    * Ячейки, отвечающие за пересечение одинаковых слов, имеют значение частоты встречания этого слова в возможных ключевых фразах. 
* Слово `союза` встречается в одной и той же возможной ключевой фразе со словами:
    * `советского`, `исследование`, `космоса`, `прошли` - по одному разу, соответственно, в ячейках на пересечении этих слов значение 1;
    * `времена` - дважды (в первой и последней ключевых фразах), соответственно, в ячейках на пересечении этих слов значение 2.

Word Degree для каждого слова равняется сумме значений в столбце данного слова:
* Для слова `времена`: 2 + 1 + 2 + 1 + 1 + 1 = 8

Для наглядности, добавим в матрицу, построенную на предыдущем шаге, дополнительную строку со значениями Word Degrees.


|                  | времена | советского | союза | исследование | космоса | одной | важнейших | задач | прошли |
|------------------|---------|------------|-------|--------------|---------|-------|-----------|-------|--------|
| времена          | 2       | 1          | 2     | 1            | 1       |       |           |       | 1      |
| советского       | 1       | 1          | 1     | 1            | 1       |       |           |       |        |
| союза            | 2       | 1          | 2     | 1            | 1       |       |           |       | 1      |
| исследование     | 1       | 1          | 1     | 1            | 1       |       |           |       |        |
| космоса          | 1       | 1          | 1     | 1            | 1       |       |           |       |        |
| одной            |         |            |       |              |         | 1     |           |       |        |
| важнейших        |         |            |       |              |         |       | 1         | 1     |        |
| задач            |         |            |       |              |         |       | 1         |       |        |
| прошли           | 1       |            |       |              |         |       |           |       | 1      |
| **Word Degrees** | 8       | 5          | 8     | 5            | 5       | 1     | 2         | 2     | 3      |


Также значение Word Degree для слова равняется сумме длин всех ключевых фраз, в которых данное слово присутствует.
* Слово `времена` присутствует в следующих возможных ключевых фразах: 
`[('времена', 'советского', 'союза', 'исследование', 'космоса'), ('времена', 'союза', 'прошли')]`
    * Word Degree(`времена`) = длина(`('времена', 'советского', 'союза', 'исследование', 'космоса')`) + длина(`('времена', 'союза', 'прошли')`) = 8

Функция принимает на вход список возможных ключевых фраз и список знаменательных слов.

Возвращаемым значением функции должен быть словарь со значениями Word Degree для каждого слова со следующей структурой:

```python
{
    "content_word_1": word_degree_1,
    "content_word_2": word_degree_2,
    ...
}
```

Интерфейс:
```py
def calculate_word_degrees(candidate_keyword_phrases: KeyPhrases,
                           content_words: list[str]) -> Optional[dict[str, int]]:
    pass
```  

**Важно:** если какого-то слова из списка знаменательных слов нет в списке с возможными ключевыми фразами, то его Word Degree = 0.

### Шаг 5. Рассчитать Word Score каждого слова

Word Score - ещё одна метрика оценки, используемая для оценки важности слова в контексте документа.

Она рассчитывается как частное от Word Degree и частоты встречаемости слова:

$Word Score=\frac{Word Degree}{Word Frequency}$

Значения данной метрики будут выше для тех слов, которые не так частотны, но встречаются в длинных возможных ключевых фразах.

Напишите функцию, которая рассчитывает значение Word Score для каждого значимого слова. 

Функция принимает на вход словарь со значениями Word Degree для каждого слова и словарь Word Frequency.

Возвращаемым значением функции должен быть словарь со значениями Word Score для каждого слова со следующей структурой:

```python
{
    "content_word_1": word_score_1,
    "content_word_2": word_score_2,
    ...
}
```

Интерфейс:
```py
def calculate_word_scores(word_degrees: dict[str, int],
                          word_frequencies: dict[str, int]) -> Optional[dict[str, float]]:
    pass
```
  
### Шаг 6. Рассчитать значимость для каждой возможной ключевой фразы

Теперь, когда у нас есть значимость каждого отдельного слова, мы можем посчитать значимость каждой возможной ключевой фразы. 

Значимость возможной ключевой фразы равняется сумме значений Word Score для каждого из составляющих её слов.

**Пример**:
* Значения Word Score: `{'времена': 4.0, 'советского': 5.125, 'союза': 4.0, 'исследование': 7.65124, 'космоса': 5.0, 'одной': 1.0, 'важнейших': 2.0, 'задач': 2.0, 'прошли': 3.0}`
* Возможная ключевая фраза: `('времена', 'союза', 'прошли')`
* Значение метрики для этой возможной ключевой фразы: 4.0 + 4.0 + 3.0 = 11.0

Напишите функцию, которая рассчитывает эту метрику для каждой возможной ключевой фразы.

Функция принимает на вход список возможных ключевых фраз и словарь со значениями Word Scores.

Возвращаемым значением функции должен быть словарь со значениями метрики для каждой возможной ключевой фразы со следующей структурой:

```python
{
    ('content_word_1', 'content_word_2', ...): cumulative_score_1,
    ('content_word_4', 'content_word_2', ...): cumulative_score_2,
    ('content_word_3', 'content_word_5', ...): cumulative_score_3,
    ...
}
```

_Вопрос для самопроверки_: как вы думаете, почему в качестве типа ключевых фраз используется кортеж, а не список?

Интерфейс:
```py
def calculate_cumulative_score_for_candidates(candidate_keyword_phrases: KeyPhrases,
                                              word_scores: dict[str, float]) -> Optional[dict[KeyPhrase, float]]:
    pass
```

### Шаг 7. Получить список ключевых слов. Выполнение Шагов 1-7 соответствует 6 баллам  

Теперь, когда у нас есть значимость каждой возможной ключевой фразы, мы можем выделить из них самые значимые - настоящие ключевые фразы.
Сделать это можно с помощью сортировки полученного на предыдущем шаге словаря по значениям.

Также есть необходимость добавить ограничение на максимальную длину ключевых фраз. Это нужно для того, чтобы:
* слишком длинные ключевые фразы, которые набирают значение метрики из-за своей длины, а не значимости отдельных слов, не оказывались в топе фраз
* иметь контроль над длиной ключевых фразы

Функция принимает на вход словарь с возможными ключевыми фразами и их значимостями, число топ N необходимых ключевых фраз и 
максимальную длину ключевых фраз **в словах**, для которых будет происходить сортировка.  

Функция возвращает список N ключевых фраз по убыванию значения метрики значимости.  
Если число N больше числа возможных ключевых фраз в словаре, то возвращаются все ключевые фразы в порядке убывания значения метрики значимости.  

Для аргумента `max_length` должно выполняться, кроме указанных в начале описания, следующее правило:
* `max_length` > 0

Если это правило не выполняется, возвращается `None`.

Интерфейс:
```py
def get_top_n(keyword_phrases_with_scores: dict[KeyPhrase, float],
              top_n: int,
              max_length: int) -> Optional[list[str]]:
    pass
```    

Теперь у вас есть все инструменты для выделения настоящих ключевых фраз из текста. 
Выделите ключевые фразы (различной длины) для каждого текста из корпуса, оцените, насколько выделенные фразы отражают смысл текста.

**Важно:** выделение ключевых фраз нужно продемонстрировать в файле [`start.py`](./start.py).

### Шаг 8. Выделить ключевые фразы, содержащие стоп слова
  
Согласно Шагу №3, возможные ключевые фразы выделяются из предложений с помощью разбивки этих предложений по стоп словам.
Однако, таким образом стоп слова не включаются в возможные ключевые фразы.

**Пример**:
* Фраза: `'Во времена Советского Союза исследование космоса было одной из важнейших задач'`
* Возможные ключевые фразы: `[('времена', 'советского', 'союза', 'исследование', 'космоса'), ('одной',), ('важнейших', 'задач')]`

В данном примере `('одной',), ('важнейших', 'задач')` являются отдельными возможными ключевыми фразами, хотя словосочетание `одной из важнейших задач` [несёт больше смысла и является устойчивым](https://ruscorpora.ru/results?search=CkIKLdC%2B0LTQvdC%2B0Lkg0LjQtyDQstCw0LbQvdC10LnRiNC40YUg0LfQsNC00LDRhyIKCggIABAKGDIgCioCCAEyAQE%3D).

Для того чтобы улучшить алгоритм и позволить выделять ключевые фразы, содержащие стоп слова, нужно:
1. Найти такие возможные ключевые фразы, которые встречаются рядом *как минимум 2 раза*
    * Рядом - то есть на расстоянии одного стоп-слова
2. Найти выделенные пары рядом в токенизированных фразах
3. Создать новые возможные ключевые фразы из первой возможной ключевой фразы, найденного стоп слова и второй возможной ключевой фразы
4. Посчитать значение метрики значимости для получившихся комбинированных возможных ключевых фраз

Напишите функцию, которая находит возможные ключевые фразы, **соседствующие в одном и том же порядке** как минимум два раза в списке возможных ключевых фраз и 
строит из них новые возможные ключевые фразы, содержащие стоп слова.

Чтобы создать из соседствующих ключевых фраз новую, нужно:
1. Найти во фразах последовательность: первая часть пары, какое-то стоп слово, вторая часть пары;
2. Соединить данные части в одну возможную ключевую фразу

Функция принимает на вход список с возможными ключевыми фразами и список фраз. 

Функция возвращает список из новых возможных ключевых фраз, содержащих стоп слова.

**Пример**:
* Возможные ключевые фразы: `[('времена', 'советского', 'союза', 'исследование', 'космоса'), ('одной',), ('важнейших', 'задач'), ('времена', 'союза', 'прошли'), ..., ('одной',), ('важнейших', 'задач'), ...]`
* Соседствующие возможные ключевые фразы: `[(('одной',), ('важнейших', 'задач')), ...]`
* Фразы: `['Во времена Советского Союза исследование космоса было одной из важнейших задач', 'И именно из СССР была отправлена ракета', 'совершившая первый полет в космос', 'Произошло это 12 апреля 1961 года', ...]`
* Новые возможные ключевые слова: `[('одной', 'из', 'важнейших', 'задач'), ...]`

**Важно**: функция должна построить **все** возможные ключевые фразы. Например, для соседствующих ключевых фраз `[('ящик',), ('металла',)]` корректными будут являться:
* `('ящик', 'из', 'металла')` **и** `('ящик', 'для', 'металла')` - если они найдены в токенизированных фразах.

Интерфейс:   
```py
def extract_candidate_keyword_phrases_with_adjoining(candidate_keyword_phrases: KeyPhrases,
                                                     phrases: list[str]) -> Optional[KeyPhrases]:
    pass
```

### Шаг 9. Рассчитать значимость для каждой новой возможной ключевой фразы. Выполнение Шагов 1-9 соответствует 8 баллам

Теперь, когда новые возможные ключевые фразы готовы, нужно посчитать метрику значимости для каждой из них.

Для этого необходимо расширить уже существующую функцию `calculate_cumulative_score_for_candidates` новым аргументом и правилом.

Значение метрики значимости для каждой новой возможной ключевой фразы будет складываться из значений метрик значимости для каждой из частей, кроме стоп слов.

Функция принимает на вход список возможных ключевых фраз, словарь со значениями Word Scores и список стоп слов.

Возвращаемым значением функции должен быть словарь со значениями метрики для каждой возможной ключевой фразы со следующей структурой:

```python
{
    ('content_word_1', 'content_word_2', ...): cumulative_score_1,
    ('content_word_4', 'content_word_2', ...): cumulative_score_2,
    ('content_word_3', 'content_word_5', ...): cumulative_score_3,
    ...
}
```

Интерфейс:   
```py
def calculate_cumulative_score_for_candidates_with_stop_words(candidate_keyword_phrases: KeyPhrases,
                                                              word_scores: dict[str, float], stop_words: list[str]) -> Optional[dict[KeyPhrase, float]]:
    pass
```

Так как для стоп слов мы не считаем значения Word Score, то и в метрике значимости для возможных ключевых фраз они не должны учитываться.

Теперь вы можете рассчитать метрику значимости для новых ключевых фраз и сравнить их значимость с уже выделенными ключевыми фразами.
Выделите новые возможные ключевые фразы из каждого текста корпуса. Выполните необходимые шаги для нового списка возможных ключевых фраз и посмотрите, будут ли добавлены в топ N фразы, содержащие стоп слова. 

**Важно:** все эти шаги нужно выполнять в файле [`start.py`](./start.py).

### Шаг 10. Загрузить стоп слова из существующей базы стоп слов

Для работы RAKE необходим список стоп слов, который используется для выделения из текста возможных ключевых фраз. 
Существуют готовые списки ([1](https://www.nltk.org/search.html?q=stopwords&check_keywords=yes&area=default), [2](https://www.ranks.nl/stopwords) и др.) стоп слов для различных языков. 

Чтобы уметь выделять из текста ключевые фразы, нужно уметь работать (считывать и обрабатывать) с базами стоп слов. 

Напишите функцию, которая считывает и сохраняет в переменную базу стоп слов. База стоп слов хранится в [`.json`](https://www.json.org/json-en.html) файле.

Функция принимает на вход путь до файла, в котором хранятся списки стоп слов.

Возвращаемым значением функции должен быть словарь, в котором ключами являются названия языков, а значениями - списки стоп слов.

Интерфейс:   
```py
def load_stop_words(path: Path) -> Optional[dict[str, list[str]]]:
    pass
```

В папке [assets](./assets) вы найдёте файл [stopwords.json](./assets/stopwords.json), содержащий списки стоп слов для различных языков.

**Подсказка:** для работы с файлом [stopwords.json](./assets/stopwords.json) рекомендуется использовать стандартный модуль `json`, 
содержащий функции `dump` и `load` для записи и чтения `.json` файлов. Больше вы можете узнать в [официальной документации](https://docs.python.org/3/library/json.html).

### Шаг 11. Выделить ключевые фразы из текста на польском языке

Теперь, когда вы можете загрузить базу стоп слов для различных языков, вы можете выделить ключевые фразы из различных языков.

Используя базу стоп слов, выделите из текста на польском языке (файл [`polish.txt`](./assets/polish.txt) в папке `assets`) ключевые фразы.

**Важно:** все эти шаги нужно выполнять в файле [`start.py`](./start.py).

### Шаг 12. Построить список стоп слов для текста

Что делать, если списка стоп слов в базе ещё нет? Например, при работе с незнакомым языком, для которого не составлен список стоп слов.

Нужно сказать, что не существует точного списка стоп слов для каждого из языков, он [составляется под каждую конкретную задачу](https://en.wikipedia.org/wiki/Stop_word).

Напишите функцию, которая выделяет из текста стоп слова на основе частоты их встречания.

* [Процентиль](https://en.wikipedia.org/wiki/Percentile) - это значение в выборке данных, ниже которого по частоте находится N-процентов данных, включая само значение.
    * Медиана или 50-ый процентиль - значение, ниже которого находится 50% данных
    * 25-ый процентиль - значение, ниже которого находится 25% данных
    * 67-ый процентиль - значение, ниже которого находится 67% данных
    * и т.д.

**Пример** для частот слов:
* Словарь частот:
```python
frequencies = {'ракеты': 10, 
               'космос': 12, 
               'орбита': 2, 
               'комбинация': 4, 
               'корабль': 7, 
               'за': 35, 
               'но': 79, 
               'миллион': 3, 
               'система': 9, 
               'же': 44}
``` 

Чтобы найти стоп слова, нам нужно найти некоторый процент самых частотных слов:
1. Найти процентиль частот 
2. Выбрать те слова, частота которых выше найденного процентиля
3. Найденные слова будут являться самыми частотными - стоп словами

Найдём 80-ый процентиль для представленного выше словаря частот:
* Возьмём все частоты: `(10, 12, 2, 4, 7, 35, 79, 3, 9, 44)`
* Отсортируем их по возрастанию: `(2, 3, 4, 7, 9, 10, 12, 35, 44, 79)`
* Так как количество элементов равно 10, то 80-ый процентиль находится на 8-ой позиции и равен 35

Теперь, когда известен 80-ый процентиль, мы можем найти слова, которые имеют такую же (35) или большую частоту:
* `['за', 'но', 'же']` - для этого распределения частотностей данные слова будут являться стоп словами, если в качестве критерия принадлежности к стоп словам выбирается 80-й процентиль.

Напишите функцию, которая выделяет из текста стоп слова.

Функция принимает на вход сырой текст в виде строки и максимальную длину (в символах) для стоп слов.

**Важно:** максимальная длина необходима, чтобы была возможность фильтровать слишком длинные слова (характерные для текста), которые встречаются большое количество раз.

**Важно:** используйте 80-ый процентиль в качестве границы для выделения стоп слов.

Функция возвращает список из стоп слов.

Интерфейс:   
```py
def generate_stop_words(text: str,
                        max_length: int) -> Optional[list[str]]:
    pass
```

Данный список стоп слов можно использовать для алгоритма RAKE при выделении ключевых слов.

### Шаг 13. Выделить из неизвестного текста стоп слова и выделить ключевые фразы. Выполнение Шагов 1-13 соответствует 10 баллам

В папке [assets](./assets) вы найдёте файл [unknown.txt](./assets/unknown.txt), содержащий текст на неизвестном языке, из которого вам нужно выделить ключевые фразы. 
Списка стоп слов для языка у вас нет.

У вас есть инструмент для автоматического выделения стоп слов и алгоритм, выделяющий ключевые фразы с помощью стоп слов. 
Продемонстрируйте использование этих подходов на тексте, написанном на неизвестном языке. Определите, используя Интернет и ваши лингвистические способности, на каком языке написан неизвестный текст.

**Важно:** все эти шаги нужно выполнять в файле [`start.py`](./start.py).

## Узнать больше

* [Ключевые слова](https://en.wikipedia.org/wiki/Index_term)
* [Оригинальная статья, описывающая RAKE](https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents)
* [Что такое JSON](https://www.json.org/json-en.html)
